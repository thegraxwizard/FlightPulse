{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e32f0d0-fe0f-48bf-aca3-f56967b5cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SparkSession Created ---\n",
      "--- Base DataFrame Prepared and Cache Cleared ---\n",
      "--- Feature Engineering Complete ---\n",
      "--- Model Trained and Predictions Made ---\n",
      "\n",
      "--- Final, Realistic Model Performance ---\n",
      "Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  758|\n",
      "|    0|       0.0| 1163|\n",
      "+-----+----------+-----+\n",
      "\n",
      "Accuracy: 60.54%\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# === FINAL DEFINITIVE SCRIPT (VERSION 3 - CORRECTED) =============\n",
    "# ===================================================================\n",
    "\n",
    "# === Imports ===\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, dayofweek\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# === Step 1: Create Spark Session ===\n",
    "spark = SparkSession.builder.appName(\"FlightPulse-Final-ML-V3\").getOrCreate()\n",
    "print(\"--- SparkSession Created ---\")\n",
    "\n",
    "\n",
    "# === Step 2: Load and Prepare Base DataFrame ===\n",
    "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True)\n",
    "df_transformed = df.withColumn(\n",
    "    \"flight_date_formatted\", \n",
    "    to_date(col(\"FLIGHT_DATE\"), \"dd-MM-yyyy\")\n",
    ").withColumn(\n",
    "    \"day_of_week\",\n",
    "    dayofweek(col(\"flight_date_formatted\"))\n",
    ")\n",
    "df_transformed.unpersist()\n",
    "print(\"--- Base DataFrame Prepared and Cache Cleared ---\")\n",
    "\n",
    "\n",
    "# === Step 3: Final Feature Engineering (DEFINITIVELY NO LEAKS) ===\n",
    "df_with_label = df_transformed.withColumn(\"label\", (col(\"HAUL\") == \"LONG\").cast(\"integer\"))\n",
    "\n",
    "categorical_cols = [\"TIME_OF_DAY\"] \n",
    "numerical_cols = [\"day_of_week\"]\n",
    "\n",
    "# --- THIS IS THE CORRECTED PART ---\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "assembler_inputs = [f\"{c}_index\" for c in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "ml_ready_data = pipeline.fit(df_with_label).transform(df_with_label)\n",
    "print(\"--- Feature Engineering Complete ---\")\n",
    "\n",
    "\n",
    "# === Step 4: Split, Train, and Predict ===\n",
    "(training_data, testing_data) = ml_ready_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(training_data)\n",
    "predictions = lr_model.transform(testing_data)\n",
    "print(\"--- Model Trained and Predictions Made ---\")\n",
    "\n",
    "\n",
    "# === Step 5: Final Evaluation ===\n",
    "correct_predictions = predictions.filter(\"label == prediction\").count()\n",
    "total_predictions = predictions.count()\n",
    "\n",
    "if total_predictions > 0:\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "else:\n",
    "    accuracy = 0\n",
    "\n",
    "print(\"\\n--- Final, Realistic Model Performance ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "predictions.groupBy(\"label\", \"prediction\").count().show()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c7eace-f56f-40a3-a9f0-d999b145cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Decision Tree Model ---\n",
      "Decision Tree Accuracy: 60.54%\n",
      "Decision Tree Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  758|\n",
      "|    0|       0.0| 1163|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "--- Training Random Forest Model ---\n",
      "Random Forest Accuracy: 60.54%\n",
      "Random Forest Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  758|\n",
      "|    0|       0.0| 1163|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "# We will reuse our 'training_data' and 'testing_data' from before.\n",
    "total_predictions = testing_data.count()\n",
    "\n",
    "# --- Train a Decision Tree Model ---\n",
    "print(\"--- Training Decision Tree Model ---\")\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n",
    "dt_model = dt.fit(training_data)\n",
    "dt_predictions = dt_model.transform(testing_data)\n",
    "\n",
    "dt_correct_predictions = dt_predictions.filter(\"label == prediction\").count()\n",
    "dt_accuracy = dt_correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy * 100:.2f}%\")\n",
    "print(\"Decision Tree Confusion Matrix:\")\n",
    "dt_predictions.groupBy(\"label\", \"prediction\").count().show()\n",
    "\n",
    "\n",
    "# --- Train a Random Forest Model ---\n",
    "print(\"\\n--- Training Random Forest Model ---\")\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "rf_model = rf.fit(training_data)\n",
    "rf_predictions = rf_model.transform(testing_data)\n",
    "\n",
    "rf_correct_predictions = rf_predictions.filter(\"label == prediction\").count()\n",
    "rf_accuracy = rf_correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "rf_predictions.groupBy(\"label\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc8fa6f8-0859-4bad-9d53-76697bec786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random Forest Performance with NEW Feature ---\n",
      "New Accuracy: 100.00%\n",
      "New Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0| 1116|\n",
      "|    1|       1.0|  805|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, concat_ws, col\n",
    "\n",
    "# --- 1. Engineer a New Feature: Average Seats per Route ---\n",
    "\n",
    "# First, create a 'route' and 'total_seats' column on our main df_transformed DataFrame\n",
    "df_with_seats = df_transformed.withColumn(\n",
    "    \"route\", \n",
    "    concat_ws(\"-\", col(\"DEPARTURE_STATION_CD\"), col(\"ARRIVAL_STATION_CD\"))\n",
    ").withColumn(\n",
    "    \"total_seats\", \n",
    "    col(\"FIRST_CLASS_SEATS\") + col(\"BUSINESS_CLASS_SEATS\") + col(\"ECONOMY_SEATS\")\n",
    ")\n",
    "\n",
    "# Now, calculate the average seats for each route from this new DataFrame\n",
    "avg_seats_per_route = df_with_seats.groupBy(\"route\").agg(avg(\"total_seats\").alias(\"avg_seats_for_route\"))\n",
    "\n",
    "# --- THIS IS THE CORRECTED PART ---\n",
    "# Perform a simpler, more robust join using the 'route' column which now exists in both DataFrames\n",
    "df_with_new_feature = df_with_seats.join(avg_seats_per_route, on=\"route\")\n",
    "\n",
    "# Create the label for prediction\n",
    "df_with_label_final = df_with_new_feature.withColumn(\"label\", (col(\"HAUL\") == \"LONG\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "# --- 2. Retrain the Models with the New Feature ---\n",
    "categorical_cols = [\"TIME_OF_DAY\"] \n",
    "numerical_cols = [\"day_of_week\", \"avg_seats_for_route\"] # <-- NEW FEATURE ADDED\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "assembler_inputs = [f\"{c}_index\" for c in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "ml_ready_data_new = pipeline.fit(df_with_label_final).transform(df_with_label_final)\n",
    "\n",
    "# Split the new data and retrain our Random Forest model\n",
    "(training_data_new, testing_data_new) = ml_ready_data_new.randomSplit([0.8, 0.2], seed=42)\n",
    "total_predictions_new = testing_data_new.count()\n",
    "\n",
    "rf_new = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "rf_model_new = rf_new.fit(training_data_new)\n",
    "rf_predictions_new = rf_model_new.transform(testing_data_new)\n",
    "\n",
    "rf_correct_new = rf_predictions_new.filter(\"label == prediction\").count()\n",
    "rf_accuracy_new = rf_correct_new / total_predictions_new if total_predictions_new > 0 else 0\n",
    "\n",
    "print(\"--- Random Forest Performance with NEW Feature ---\")\n",
    "print(f\"New Accuracy: {rf_accuracy_new * 100:.2f}%\")\n",
    "print(\"New Confusion Matrix:\")\n",
    "rf_predictions_new.groupBy(\"label\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddf144-0924-490b-9121-5a476ba2cb20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
